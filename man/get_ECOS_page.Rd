% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/species_page.R
\name{get_ECOS_page}
\alias{get_ECOS_page}
\title{Use phantomjs to get a page from ECOS}
\usage{
get_ECOS_page(url = NULL, file = NULL, wait = TRUE)
}
\arguments{
\item{url}{The URL of the species' ECOS page}

\item{file}{The path to which the HTML will be written}
}
\value{
The status of the phantomjs scrape (1 or 0) or NULL if an HTTP error
}
\description{
Use phantomjs to get a page from ECOS
}
\details{
A significant amount of content on ECOS pages is rendered from 
javascript functions, and 'standard' scraping tools like \link[xml2]{read_html}
miss those components. \href{https://github.com/ariya/phantomjs}{phantomjs}
provides a function (\code{phantomjs}) to render the full page, complete with
javascript elements, and write them to file. 

This function should be preferred over \link{get_species_page} in most cases 
even though it is slightly slower with the scrape because it will get certain
tables (e.g., the petitions table) and content like the species' range map
data.
}
\examples{
\dontrun{
   An example, not run
}
}
